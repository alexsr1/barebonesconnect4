{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexsr1/barebonesconnect4/blob/main/nb/Meta_Synthetic_Data_Llama3_2_(3B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP_t5JqWcfpb"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "\n",
        "\n",
        "<a href=\"https://github.com/meta-llama/synthetic-data-kit\"><img src=\"https://raw.githubusercontent.com/unslothai/notebooks/refs/heads/main/assets/meta%20round%20logo.png\" width=\"137\"></a>\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om2qjxs5PSr0"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCa86oMuPSr0"
      },
      "source": [
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imwJhjjYPSr0"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UWzENK7_cfpk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm synthetic-data-kit==0.0.3\n",
        "else:\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade         unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "    !uv pip install synthetic-data-kit==0.0.3\n",
        "!uv pip install transformers==4.56.2\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X_PLdvSJcfpn"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.56.2\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm1OJo9ocfpo"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuQRLCikcfpp"
      },
      "source": [
        "## Primary Goal\n",
        "Our goal is to make Llama 3.2 3B understand the \"Byte Latent Transformer: Patches Scale Better Than Tokens\" [research paper](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/) that was published in December 2024.\n",
        "\n",
        "We'll use https://github.com/meta-llama/synthetic-data-kit to generate question and answer pairs **fully locally** which will be used for finetuning Llama 3.2 3B!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bf355090a6e142d38380f15b6505f2ef",
            "38f2608fbba44cb3b32074f7f6ea39a1",
            "011f35ccc7dc4b83be32265266d7c978",
            "6886373a745c43b8a4558c6761c8e8f8",
            "ebf8b4b97d464ee1872713fcf637fc1b",
            "e5ad5aa9f4dd4da6937b32255d5fcdde",
            "f418bb36d5724366a81bcd5e44a5b6aa",
            "bcecb460e3254424b2eda65376f29fbb",
            "45368545bf644d8e9724f95f3cad7503",
            "eb7f86d0ff1d4b76863d1e3c3b408263",
            "5cc9a1e6523a4b1ebf4ec7ca0cb0d8c7",
            "0ff4c1d1ed0c4317bb147a9af0852dcd",
            "625a537669454f3cbc4fa93a0ca0a196",
            "2af14837a0a04742bc8e7b52a1624e8f",
            "2d99982e163c495699f42edabf5f3668",
            "65015e89e1d34daaa738a5d4b38ea879",
            "8f64b0404e544959a6501b7c6a91d2fa",
            "1447b6a2d32a450c8f8d24fe748b3c5b",
            "1eda89fcaeae4ecc9c347be56be61308",
            "134ee758528e461f99cb2e9f4c9a5709",
            "656c2c3e7ec8495e88c2f63cfcc1af30",
            "27549249c0d74aac928aaa21b686ea14",
            "6ac948987aef4f00b880bff527cf766b",
            "abbb236ea25d47cfb4ebb5a7b1b46597",
            "c1a7d5d7bcb944fdad8a06c148cfeb7f",
            "441f891f7f384313b423a2a844bb6b3d",
            "1718df89e2b0473fb1377dca84bb4b19",
            "81eb748438dd4ebab05f1931e6f8afe7",
            "3d6dc8c5e6be40e0a6316bf77145338a",
            "d6a40990f1614123a133c39ff82773ed",
            "051dc27e78f2472b94f4bde15b97fd2f",
            "32b5068d99004963b0ff57aad5b31404",
            "31c27cc627af4493ac0924ae2a53f361",
            "60383438655a4535b5374fbe14b984da",
            "347f444baaec4a54adbd53f1cff082f6",
            "b184d42ac2c84e87b7a8e9632fcbb9ed",
            "4c70b289d21245a9a892dbb2a298d9ff",
            "b698b397e90f4c6a86970cbfe4b21ce2",
            "73d4732077f04c9990bcb66f609397b1",
            "92fbcf9ebb3b4ca5ba80e8c095b3efac",
            "fb66a0d400db4453a2123007c0facedb",
            "06a9e9917f6741e381a201079c627fdd",
            "0b50def3a36d4cf99326f571ea40cbed",
            "b07abf3bab734e44a51bad313c375331",
            "ad069762361346beaae8c9906828cb9c",
            "7e826cb8408c44f9b79158d80178222a",
            "a0abd1d66690420f86b3bb2f118317ad",
            "0489ba3462a648a6ad6a24561943a3de",
            "dfd0069cde014b76a9e6f0bace77c581",
            "5b9803bbb8164596a9c3182a23698339",
            "3af862958cd8493185a753ee311a6cb3",
            "319f32e7b8de4adf94cf2d85a92c15fb",
            "43dff7b158a04c32a32ed8e52e17f19e",
            "887de74451c347a89c99b87d02bd9bc9",
            "5ff34b38d9074e2a8f8f0fe83e14a482"
          ]
        },
        "id": "Ym8UA1PRiXsa",
        "outputId": "bebbd135-6630-4165-a1d3-8afc5e5eeb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-25 08:20:54 [__init__.py:244] Automatically detected platform cuda.\n",
            "ERROR 10-25 08:21:01 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf355090a6e142d38380f15b6505f2ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ff4c1d1ed0c4317bb147a9af0852dcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ac948987aef4f00b880bff527cf766b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60383438655a4535b5374fbe14b984da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad069762361346beaae8c9906828cb9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-25 08:21:11 [vllm_utils.py:694] Unsloth: Patching vLLM v1 graph capture\n",
            "INFO 10-25 08:21:11 [vllm_utils.py:722] Unsloth: Patching vLLM v0 graph capture\n",
            "Unsloth: Using dtype = torch.float16 for vLLM.\n",
            "Unsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 89.39%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 192.\n",
            "Unsloth: vLLM's KV Cache can use up to 7.19 GB. Also swap space = 0 GB.\n",
            "vLLM STDOUT: INFO 10-25 08:21:21 [__init__.py:244] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 10-25 08:21:24 [api_server.py:1395] vLLM API server version 0.9.2\n",
            "vLLM STDOUT: INFO 10-25 08:21:24 [cli_args.py:325] non-default args: {'model': 'unsloth/Llama-3.2-3B-Instruct', 'dtype': 'float16', 'seed': 0, 'max_model_len': 2048, 'max_logprobs': 0, 'gpu_memory_utilization': 0.8938626454842437, 'swap_space': 0.0, 'enable_prefix_caching': True, 'max_num_batched_tokens': 2048, 'max_num_seqs': 192, 'enable_chunked_prefill': True, 'disable_log_stats': True}\n",
            "vLLM STDOUT: INFO 10-25 08:21:39 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
            "vLLM STDOUT: WARNING 10-25 08:21:39 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
            "vLLM STDOUT: INFO 10-25 08:21:39 [config.py:1472] Using max model len 2048\n",
            "vLLM STDOUT: WARNING 10-25 08:21:39 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "vLLM STDOUT: INFO 10-25 08:21:41 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "vLLM STDOUT: INFO 10-25 08:21:41 [api_server.py:268] Started engine process with PID 2848\n",
            "vLLM STDOUT: INFO 10-25 08:21:53 [__init__.py:244] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 10-25 08:21:55 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='unsloth/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":192,\"local_cache_dir\":null}, use_cached_outputs=True, \n",
            "vLLM STDOUT: INFO 10-25 08:21:56 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "vLLM STDOUT: INFO 10-25 08:21:56 [cuda.py:360] Using XFormers backend.\n",
            "vLLM STDOUT: INFO 10-25 08:21:57 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "vLLM STDOUT: INFO 10-25 08:21:57 [model_runner.py:1171] Starting to load model unsloth/Llama-3.2-3B-Instruct...\n",
            "vLLM STDOUT: INFO 10-25 08:21:58 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
            "vLLM STDOUT: INFO 10-25 08:25:11 [weight_utils.py:308] Time spent downloading weights for unsloth/Llama-3.2-3B-Instruct: 193.045932 seconds\n",
            "vLLM STDOUT: INFO 10-25 08:25:39 [default_loader.py:272] Loading weights took 28.19 seconds\n",
            "vLLM STDOUT: INFO 10-25 08:25:40 [model_runner.py:1203] Model loading took 6.0160 GiB and 222.163406 seconds\n",
            "vLLM STDOUT: INFO 10-25 08:25:42 [worker.py:294] Memory profiling takes 1.60 seconds\n",
            "vLLM STDOUT: INFO 10-25 08:25:42 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.89) = 13.18GiB\n",
            "vLLM STDOUT: INFO 10-25 08:25:42 [worker.py:294] model weights take 6.02GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 0.90GiB; the rest of the memory reserved for KV Cache is 6.22GiB.\n",
            "vLLM STDOUT: INFO 10-25 08:25:42 [executor_base.py:113] # cuda blocks: 3637, # CPU blocks: 0\n",
            "vLLM STDOUT: INFO 10-25 08:25:42 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 28.41x\n",
            "vLLM STDOUT: INFO 10-25 08:25:42 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "vLLM STDOUT: INFO 10-25 08:26:06 [model_runner.py:1671] Graph capturing finished in 24 secs, took 0.15 GiB\n",
            "vLLM STDOUT: INFO 10-25 08:26:06 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 26.11 seconds\n",
            "vLLM STDOUT: WARNING 10-25 08:26:07 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:29] Available routes are:\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /docs, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /redoc, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /health, Methods: GET\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /load, Methods: GET\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /ping, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /ping, Methods: GET\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /tokenize, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /detokenize, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /v1/models, Methods: GET\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /version, Methods: GET\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /v1/completions, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /v1/embeddings, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /pooling, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /classify, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /score, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /v1/score, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /v1/audio/translations, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /v1/rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /v2/rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /invocations, Methods: POST\n",
            "vLLM STDOUT: INFO 10-25 08:26:07 [launcher.py:37] Route: /metrics, Methods: GET\n",
            "vLLM Server Ready Detected\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33428 - \"GET /metrics HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "from unsloth.dataprep import SyntheticDataKit\n",
        "\n",
        "generator = SyntheticDataKit.from_pretrained(\n",
        "    # Choose any model from https://huggingface.co/unsloth\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048, # Longer sequence lengths will be slower!\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef_MnK575tr2"
      },
      "source": [
        "## Generate QA Pairs + Auto clean data\n",
        "We now use synthetic data kit for question answer pair generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q487TNby-nwT"
      },
      "outputs": [],
      "source": [
        "generator.prepare_qa_generation(\n",
        "    output_folder = \"data\", # Output location of synthetic data\n",
        "    temperature = 0.7, # Higher temp makes more diverse datases\n",
        "    top_p = 0.95,\n",
        "    overlap = 64, # Overlap portion during chunking\n",
        "    max_generation_tokens = 512, # Can increase for longer QA pairs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV7DyufR51IN"
      },
      "source": [
        "Check if it succeeded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2gQZcr_Wp94",
        "outputId": "45c63eea-7ad0-461a-aaa9-1d5ca5e6d6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:33430 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[?25l\u001b[32m VLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[32m‚†ã\u001b[0m\u001b[32m Checking VLLM server at http://localhost:8000/v1...\u001b[0m\r\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1761380769\u001b[0m, \n",
            "\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
            "\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m2048\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'modelperm-50dbe95bae364ce2910fd24b985ae333'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n",
            "\u001b[32m'created'\u001b[0m: \u001b[1;36m1761380769\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n",
            "\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
            "\u001b[32m‚†ã\u001b[0m Checking VLLM server at http://localhost:8000/v1...\r\u001b[2K\u001b[32m‚†ã\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n",
            "\u001b[?25h\r\u001b[1A\u001b[2K"
          ]
        }
      ],
      "source": [
        "!synthetic-data-kit system-check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdl7aPFK55M1"
      },
      "source": [
        "## Document Parsing (PDF, CSV, HTML etc.)\n",
        "Now, let's take the Byte Latent Transformer: Patches Scale Better Than Tokens research paper found at https://arxiv.org/abs/2412.09871 and covert it to Q&A pairs in order to finetune Llama 3.2!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BN1yrPGmANA",
        "outputId": "a7a1ee39-88e6-4a68-eb0d-350c62141132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K\u001b[32m‚†º\u001b[0m Processing https://arxiv.org/html/2412.09871v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "34 ['data/output/arxiv_org_0.txt', 'data/output/arxiv_org_1.txt', 'data/output/arxiv_org_2.txt']\n"
          ]
        }
      ],
      "source": [
        "# Byte Latent Transformer: Patches Scale Better Than Tokens paper in HTML format\n",
        "!synthetic-data-kit \\\n",
        "    -c synthetic_data_kit_config.yaml \\\n",
        "    ingest \"https://arxiv.org/html/2412.09871v1\"\n",
        "\n",
        "# Truncate document\n",
        "filenames = generator.chunk_data(\"data/output/arxiv_org.txt\")\n",
        "print(len(filenames), filenames[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGdAXafV6S2M"
      },
      "source": [
        "We see around 37 chunks of data. We now call synthetic-data-kit to create some pairs of data for 3 of our chunks.\n",
        "\n",
        "You can process more chunks, but it'll be much slower!\n",
        "\n",
        "Using `--num-pairs` will generate **approximately** that many QA pairs. However it might be shorter or longer depending on the `max_seq_length` of the loaded up model. So if you specify 100, you might only get 10 since the model's max sequence length is capped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYYYlMJ7ZtT7",
        "outputId": "0d3dedfd-060c-4cd8-efcb-7c9384de687c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:33440 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33450 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m‚†¶\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO 10-25 08:26:13 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
            "vLLM STDOUT: INFO 10-25 08:26:13 [logger.py:43] Received request chatcmpl-05ee3d4cf6db4e5e87293128bda1bc04: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nByte Latent Transformer: Patches Scale Better Than Tokens\\n1 Introduction\\n2 Patching: From Individual Bytes to Groups of Bytes\\n2.1 Strided Patching Every K Bytes\\n2.2 Space Patching\\n2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM\\n2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching\\n3 BLT Architecture\\n3.1 Latent Global Transformer Model\\n3.2 Local Encoder\\n3.2.1 Encoder Hash n-gram Embeddings\\n3.2.2 Encoder Multi-Headed Cross-Attention\\n3.3 Local Decoder\\n3.3.1 Decoder Multi-headed Cross-Attention\\n4 Experimental Setup\\n4.1 Pre-training Datasets\\n4.2 Entropy Model\\n4.3 Entropy Threshold and Equalizing Context Length\\n4.4 Entropy Model Context\\n4.5 FLOPs Estimation\\n4.6 Bits-Per-Byte Estimation\\n4.7 Transformer Architecture Hyperparameters\\n4.8 BLT-Specific Hyperparameters\\n5 Scaling Trends\\n5.1 Parameter Matched Compute Optimal Scaling Trends\\n5.2 Beyond Compute Optimal Task Evaluations\\nClassification tasks\\nCoding related generation tasks:\\n5.3 Patches Scale Better Than Tokens\\n6 Byte Modeling Improves Robustness\\n6.1 Character-Level Tasks\\nNoisy Data\\nPhonology - Grapheme-to-Phoneme (G2P)\\nCUTE\\nLow Resource Machine Translation\\n6.2 Training BLT from Llama 3\\n7 Ablations and Discussion\\nEntropy Model Hyper-parameters\\nTypes of Patching\\nCross-Attention\\nn-gram Hash Embeddings\\nLocal Model Hyperparamaters\\n8 Related Work\\n9 Limitations and Future Work\\n10 Conclusion\\nCore Contributors:\\nCore Advising Group:\\nAdvisors and Contributors:\\n11 Model Hyper Parameters\\n12 FLOPs Equations\\n13 Rolling Polynomial Hashing\\n14 Frequency-based n-gram Embedddings\\n15 Entropy Patching Example from MMLU\\n]FAIR at Meta\\n1]Paul G. Allen School of Computer Science & Engineering, University of Washington\\n2]University of Chicago\\n\\\\contribution\\n[‚Ä°]Joint second author\\n\\\\contribution[‚Ä†]Joint last author\\n\\\\contribution[‚ãÑ]Work done at Meta\\nByte Latent Transformer: Patches Scale Better Than Tokens\\nArtidoro Pagnoni\\nRam Pasunuru\\nPedro Rodriguez\\nJohn Nguyen\\nBenjamin Muller\\nMargaret Li\\nChunting Zhou\\nLili Yu\\nJason Weston\\nLuke Zettlemoyer\\nGargi Ghosh\\nMike Lewis\\nAri Holtzman\\nSrinivasan Iyer\\n[\\n[\\n[\\ncs.washington.edu\\nmeta.com\\n(July 25, 2025)\\nAbstract\\nWe introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness.\\nBLT encodes bytes into dynamically sized patches, which serve as the primary units of computation.\\nPatches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it.\\nWe present the first flop controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary.\\nBoth training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.\\n\\\\correspondence\\nartidoro at, sviyer at\\n\\\\metadata\\n[Code]https://github.com/facebookresearch/blt\\n1 Introduction\\nFigure 1:\\nScaling trends for fixed inference flop models (fully) trained with varying training budgets.\\nIn token-based models, a fixed inference budget determines the model size.\\nIn contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama\\xa02 and<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-25 08:26:13 [engine.py:317] Added request chatcmpl-05ee3d4cf6db4e5e87293128bda1bc04.\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO:     127.0.0.1:33456 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-25 08:26:40 [logger.py:43] Received request chatcmpl-7ef833285c9b4b38a4fd16d928146172: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nByte Latent Transformer: Patches Scale Better Than Tokens\\n1 Introduction\\n2 Patching: From Individual Bytes to Groups of Bytes\\n2.1 Strided Patching Every K Bytes\\n2.2 Space Patching\\n2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM\\n2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching\\n3 BLT Architecture\\n3.1 Latent Global Transformer Model\\n3.2 Local Encoder\\n3.2.1 Encoder Hash n-gram Embeddings\\n3.2.2 Encoder Multi-Headed Cross-Attention\\n3.3 Local Decoder\\n3.3.1 Decoder Multi-headed Cross-Attention\\n4 Experimental Setup\\n4.1 Pre-training Datasets\\n4.2 Entropy Model\\n4.3 Entropy Threshold and Equalizing Context Length\\n4.4 Entropy Model Context\\n4.5 FLOPs Estimation\\n4.6 Bits-Per-Byte Estimation\\n4.7 Transformer Architecture Hyperparameters\\n4.8 BLT-Specific Hyperparameters\\n5 Scaling Trends\\n5.1 Parameter Matched Compute Optimal Scaling Trends\\n5.2 Beyond Compute Optimal Task Evaluations\\nClassification tasks\\nCoding related generation tasks:\\n5.3 Patches Scale Better Than Tokens\\n6 Byte Modeling Improves Robustness\\n6.1 Character-Level Tasks\\nNoisy Data\\nPhonology - Grapheme-to-Phoneme (G2P)\\nCUTE\\nLow Resource Machine Translation\\n6.2 Training BLT from Llama 3\\n7 Ablations and Discussion\\nEntropy Model Hyper-parameters\\nTypes of Patching\\nCross-Attention\\nn-gram Hash Embeddings\\nLocal Model Hyperparamaters\\n8 Related Work\\n9 Limitations and Future Work\\n10 Conclusion\\nCore Contributors:\\nCore Advising Group:\\nAdvisors and Contributors:\\n11 Model Hyper Parameters\\n12 FLOPs Equations\\n13 Rolling Polynomial Hashing\\n14 Frequency-based n-gram Embedddings\\n15 Entropy Patching Example from MMLU\\n]FAIR at Meta\\n1]Paul G. Allen School of Computer Science & Engineering, University of Washington\\n2]University of Chicago\\n\\\\contribution\\n[‚Ä°]Joint second author\\n\\\\contribution[‚Ä†]Joint last author\\n\\\\contribution[‚ãÑ]Work done at Meta\\nByte Latent Transformer: Patches Scale Better Than Tokens\\nArtidoro Pagnoni\\nRam Pasunuru\\nPedro Rodriguez\\nJohn Nguyen\\nBenjamin Muller\\nMargaret Li\\nChunting Zhou\\nLili Yu\\nJason Weston\\nLuke Zettlemoyer\\nGargi Ghosh\\nMike Lewis\\nAri Holtzman\\nSrinivasan Iyer\\n[\\n[\\n[\\ncs.washington.edu\\nmeta.com\\n(July 25, 2025)\\nAbstract\\nWe introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness.\\nBLT encodes bytes into dynamically sized patches, which serve as the primary units of computation.\\nPatches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it.\\nWe present the first flop controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary.\\nBoth training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.\\n\\\\correspondence\\nartidoro at, sviyer at\\n\\\\metadata\\n[Code]https://github.com/facebookresearch/blt\\n1 Introduction\\nFigure 1:\\nScaling trends for fixed inference flop models (fully) trained with varying training budgets.\\nIn token-based models, a fixed inference budget determines the model size.\\nIn contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama\\xa02 and<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m‚†¶\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO 10-25 08:26:40 [engine.py:317] Added request chatcmpl-7ef833285c9b4b38a4fd16d928146172.\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO:     127.0.0.1:44144 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 9 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†ã\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:58262 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[?25lvLLM STDOUT: INFO:     127.0.0.1:58264 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-25 08:27:01 [logger.py:43] Received request chatcmpl-06ce017bcb164b8c9131c0d88a829df4: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\na fixed inference budget determines the model size.\\nIn contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama\\xa02 and 3. Moving to the larger inference budget makes the larger patch size 8 model more desirable sooner. Both BPE compute-optimal point and crossover point are indicated with vertical lines.\\nFigure 2:\\nBLT comprises three modules, a lightweight Local Encoder that encodes input bytes into patch representations, a computationally expensive Latent Transformer over patch representations, and a lightweight Local Decoder to decode the next patch of bytes. BLT incorporates byte nnitalic_n-gram embeddings and a cross-attention mechanism to maximize information flow between the Latent Transformer and the byte-level modules\\xa0(Figure\\xa05). Unlike fixed-vocabulary tokenization, BLT dynamically groups bytes into patches preserving access to the byte-level information.\\nWe introduce the Byte Latent Transformer\\xa0(BLT), a tokenizer-free architecture that learns from raw byte data and, for the first time, matches the performance of tokenization-based models at scale, with significant improvements in efficiency and robustness (¬ß6).\\nExisting large language models (llms) are trained almost entirely end-to-end, except for tokenization‚Äîa heuristic pre-processing step that groups bytes into a static set of tokens.\\nSuch tokens bias how a string is compressed, leading to shortcomings such as domain/modality sensitivity\\xa0(Dagan et\\xa0al., 2024), sensitivity to input noise\\xa0(¬ß6), a lack of orthographic knowledge\\xa0(Edman et\\xa0al., 2024), and multilingual inequity\\xa0(Liang et\\xa0al., 2023; Petrov et\\xa0al., 2024; Limisiewicz et\\xa0al., 2024).\\nTokenization has previously been essential because directly training llms on bytes is prohibitively costly at scale due to long sequence lengths\\xa0(Xue et\\xa0al., 2022).\\nPrior works mitigate this by employing more efficient self-attention\\xa0(El\\xa0Boukkouri et\\xa0al., 2020; Clark et\\xa0al., 2022) or attention-free architectures\\xa0(Wang et\\xa0al., 2024)\\xa0(¬ß8). However, this primarily helps train small models.\\nAt scale, the computational cost of a Transformer is dominated by large feed-forward network layers that run on every byte, not the cost of the attention mechanism.\\nTo efficiently allocate compute, we propose a dynamic, learnable method for grouping bytes into patches\\xa0(¬ß2) and a new model architecture that mixes byte and patch information.\\nUnlike tokenization, BLT has no fixed vocabulary for patches.\\nArbitrary groups of bytes are mapped to latent patch representations via light-weight learned encoder and decoder modules.\\nWe show that this results in more efficient allocation of compute than tokenization-based models.\\nTokenization-based llms allocate the same amount of compute to every token. This trades efficiency for performance, since tokens are induced with compression heuristics that are not always correlated with the complexity of predictions. Central to our architecture is the idea that models should dynamically allocate compute where it is needed. For example, a large transformer is not needed to predict the ending of most words, since these are comparably easy, low-entropy decisions compared to choosing the first word of a new sentence.\\nThis is reflected in BLT‚Äôs architecture\\xa0(¬ß3) where there are three transformer blocks: two small byte-level local models and a large global latent transformer\\xa0(Figure\\xa02).\\nTo determine how to group bytes into patches and therefore how to dynamically allocate compute, BLT\\nsegments data based on the entropy of the next-byte prediction creating contextualized groupings of bytes with relatively uniform information density.\\nWe present the first flop-controlled scaling study of byte-level models up to 8B parameters and 4T training bytes, showing that we can train a model end-to-end at scale from bytes without fixed-vocabulary tokenization.\\nOverall, BLT matches training flop-controlled performance111We calculate the computational cost of a model by counting the number of Floating Point OPerations (flops) needed. of Llama 3 while using up to 50% fewer flops at inference\\xa0(¬ß5).\\nWe also show that directly working with raw bytes provides significant improvements in modeling the long-tail of the data<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-25 08:27:01 [engine.py:317] Added request chatcmpl-06ce017bcb164b8c9131c0d88a829df4.\n",
            "\u001b[2K\u001b[32m‚†¶\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...vLLM STDOUT: INFO:     127.0.0.1:58266 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...vLLM STDOUT: INFO 10-25 08:27:05 [logger.py:43] Received request chatcmpl-5b1f75c85eb744ed9ce1657f77a50e65: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n a fixed inference budget determines the model size.\\nIn contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama\\xa02 and 3. Moving to the larger inference budget makes the larger patch size 8 model more desirable sooner. Both BPE compute-optimal point and crossover point are indicated with vertical lines.\\nFigure 2:\\nBLT comprises three modules, a lightweight Local Encoder that encodes input bytes into patch representations, a computationally expensive Latent Transformer over patch representations, and a lightweight Local Decoder to decode the next patch of bytes. BLT incorporates byte nnitalic_n-gram embeddings and a cross-attention mechanism to maximize information flow between the Latent Transformer and the byte-level modules\\xa0(Figure\\xa05). Unlike fixed-vocabulary tokenization, BLT dynamically groups bytes into patches preserving access to the byte-level information.\\nWe introduce the Byte Latent Transformer\\xa0(BLT), a tokenizer-free architecture that learns from raw byte data and, for the first time, matches the performance of tokenization-based models at scale, with significant improvements in efficiency and robustness (¬ß6).\\nExisting large language models (llms) are trained almost entirely end-to-end, except for tokenization‚Äîa heuristic pre-processing step that groups bytes into a static set of tokens.\\nSuch tokens bias how a string is compressed, leading to shortcomings such as domain/modality sensitivity\\xa0(Dagan et\\xa0al., 2024), sensitivity to input noise\\xa0(¬ß6), a lack of orthographic knowledge\\xa0(Edman et\\xa0al., 2024), and multilingual inequity\\xa0(Liang et\\xa0al., 2023; Petrov et\\xa0al., 2024; Limisiewicz et\\xa0al., 2024).\\nTokenization has previously been essential because directly training llms on bytes is prohibitively costly at scale due to long sequence lengths\\xa0(Xue et\\xa0al., 2022).\\nPrior works mitigate this by employing more efficient self-attention\\xa0(El\\xa0Boukkouri et\\xa0al., 2020; Clark et\\xa0al., 2022) or attention-free architectures\\xa0(Wang et\\xa0al., 2024)\\xa0(¬ß8). However, this primarily helps train small models.\\nAt scale, the computational cost of a Transformer is dominated by large feed-forward network layers that run on every byte, not the cost of the attention mechanism.\\nTo efficiently allocate compute, we propose a dynamic, learnable method for grouping bytes into patches\\xa0(¬ß2) and a new model architecture that mixes byte and patch information.\\nUnlike tokenization, BLT has no fixed vocabulary for patches.\\nArbitrary groups of bytes are mapped to latent patch representations via light-weight learned encoder and decoder modules.\\nWe show that this results in more efficient allocation of compute than tokenization-based models.\\nTokenization-based llms allocate the same amount of compute to every token. This trades efficiency for performance, since tokens are induced with compression heuristics that are not always correlated with the complexity of predictions. Central to our architecture is the idea that models should dynamically allocate compute where it is needed. For example, a large transformer is not needed to predict the ending of most words, since these are comparably easy, low-entropy decisions compared to choosing the first word of a new sentence.\\nThis is reflected in BLT‚Äôs architecture\\xa0(¬ß3) where there are three transformer blocks: two small byte-level local models and a large global latent transformer\\xa0(Figure\\xa02).\\nTo determine how to group bytes into patches and therefore how to dynamically allocate compute, BLT\\nsegments data based on the entropy of the next-byte prediction creating contextualized groupings of bytes with relatively uniform information density.\\nWe present the first flop-controlled scaling study of byte-level models up to 8B parameters and 4T training bytes, showing that we can train a model end-to-end at scale from bytes without fixed-vocabulary tokenization.\\nOverall, BLT matches training flop-controlled performance111We calculate the computational cost of a model by counting the number of Floating Point OPerations (flops) needed. of Llama 3 while using up to 50% fewer flops at inference\\xa0(¬ß5).\\nWe also show that directly working with raw bytes provides significant improvements in modeling the long-tail of the data<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-25 08:27:05 [engine.py:317] Added request chatcmpl-5b1f75c85eb744ed9ce1657f77a50e65.\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...vLLM STDOUT: INFO:     127.0.0.1:34618 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 13 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_1_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_1_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†ß\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_1_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:37800 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:37806 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-25 08:27:24 [logger.py:43] Received request chatcmpl-d77f3cb69813449c8944543c28362179: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ncomputational cost of a model by counting the number of Floating Point OPerations (flops) needed. of Llama 3 while using up to 50% fewer flops at inference\\xa0(¬ß5).\\nWe also show that directly working with raw bytes provides significant improvements in modeling the long-tail of the data. BLT models are more robust than tokenizer-based models to noisy inputs and display enhanced character level understanding abilities demonstrated on orthographic knowledge, phonology, and low-resource machine translation tasks\\xa0(¬ß6).\\nFinally, with BLT models, we can simultaneously increase model size and patch size while maintaining the same inference flop budget. Longer patch sizes, on average, save compute which can be reallocated to grow the size of the global latent transformer, because it is run less often. We conduct inference-flop controlled scaling experiments\\xa0(Figure\\xa01), and observe significantly better scaling trends than with tokenization-based architectures.\\nIn summary, this paper makes the following contributions:\\n1) We introduce BLT, a byte latent llm architecture that dynamically allocates compute to improve flop efficiency,\\n2) We show that we achieve training flop-controlled parity with Llama 3 up to 8B scale while having the option to trade minor losses in evaluation metrics for flop efficiency gains of up to 50%, 3) BLT models unlock a new dimension for scaling llms, where model size can now be scaled while maintaining a fixed-inference budget, 4) We demonstrate the improved robustness of BLT models to input noise and their awareness of sub-word aspects of input data that token-based llms miss.\\nWe release the training and inference code for BLT at\\xa0https://github.com/facebookresearch/blt.\\n2 Patching: From Individual Bytes to Groups of Bytes\\nFigure 3:\\nPatching schemes group bytes in different ways, each leading to a different number of resulting patches.\\nSince each patch is processed using a large transformer step, the number of patches directly determines the bulk of the compute expended in terms of flops.\\nThese schemes group bytes into patches by (a) striding every four bytes\\xa0(¬ß2.1) as in MegaByte\\xa0(Yu et\\xa0al., 2023), (b) tokenizing with Byte-Pair Encoding (bpe), in this case the Llama-3\\xa0(Dubey et\\xa0al., 2024) tokenizer, (c & d) entropy-based patching as in this work\\xa0(¬ß2.3), (e) patching on space-bytes\\xa0(Slagle, 2024), (f) and patching on entropy using a small CNN byte-level model with 2-byte context.\\nSegmenting bytes into patches allows BLT to dynamically allocate compute based on context.\\nFigure\\xa03 shows several different methods for segmenting bytes into patches.\\nFormally, a patching function fpf_{p}italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT segments a sequence of bytes ùíô={xi,|i=1,‚Ä¶n}\\\\boldsymbol{x}=\\\\{x_{i},|i=1,\\\\ldots n\\\\}bold_italic_x = { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, | italic_i = 1, ‚Ä¶ italic_n } of length nnitalic_n into a sequence of m<nm<nitalic_m < italic_n patches ùíë={pj|j=1,‚Ä¶,m}\\\\boldsymbol{p}=\\\\{p_{j}|j=1,\\\\ldots,m\\\\}bold_italic_p = { italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_j = 1, ‚Ä¶, italic_m } by mapping each xix_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to the set {0,1} where 1 indicates the start of a new patch.\\nFor both token-based and patch-based models, the computational cost of processing data is primarily determined by the number of steps executed by the main Transformer. In BLT, this is the number of patches needed to encode the data with a given patching function.\\nConsequently, the average size of a patch, or simply patch size, is the main factor for determining the cost of processing data during both training and inference with a given patching function\\xa0(¬ß4.5).\\nNext, we introduce three patching functions: patching with a fixed number of bytes per patch\\xa0(¬ß2<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-25 08:27:24 [engine.py:317] Added request chatcmpl-d77f3cb69813449c8944543c28362179.\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Generating qa content from data/output/arxiv_org_2.txt...vLLM STDOUT: INFO:     127.0.0.1:37818 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-25 08:27:30 [logger.py:43] Received request chatcmpl-82bbed6f81684165ab60079c47ce99b6: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n computational cost of a model by counting the number of Floating Point OPerations (flops) needed. of Llama 3 while using up to 50% fewer flops at inference\\xa0(¬ß5).\\nWe also show that directly working with raw bytes provides significant improvements in modeling the long-tail of the data. BLT models are more robust than tokenizer-based models to noisy inputs and display enhanced character level understanding abilities demonstrated on orthographic knowledge, phonology, and low-resource machine translation tasks\\xa0(¬ß6).\\nFinally, with BLT models, we can simultaneously increase model size and patch size while maintaining the same inference flop budget. Longer patch sizes, on average, save compute which can be reallocated to grow the size of the global latent transformer, because it is run less often. We conduct inference-flop controlled scaling experiments\\xa0(Figure\\xa01), and observe significantly better scaling trends than with tokenization-based architectures.\\nIn summary, this paper makes the following contributions:\\n1) We introduce BLT, a byte latent llm architecture that dynamically allocates compute to improve flop efficiency,\\n2) We show that we achieve training flop-controlled parity with Llama 3 up to 8B scale while having the option to trade minor losses in evaluation metrics for flop efficiency gains of up to 50%, 3) BLT models unlock a new dimension for scaling llms, where model size can now be scaled while maintaining a fixed-inference budget, 4) We demonstrate the improved robustness of BLT models to input noise and their awareness of sub-word aspects of input data that token-based llms miss.\\nWe release the training and inference code for BLT at\\xa0https://github.com/facebookresearch/blt.\\n2 Patching: From Individual Bytes to Groups of Bytes\\nFigure 3:\\nPatching schemes group bytes in different ways, each leading to a different number of resulting patches.\\nSince each patch is processed using a large transformer step, the number of patches directly determines the bulk of the compute expended in terms of flops.\\nThese schemes group bytes into patches by (a) striding every four bytes\\xa0(¬ß2.1) as in MegaByte\\xa0(Yu et\\xa0al., 2023), (b) tokenizing with Byte-Pair Encoding (bpe), in this case the Llama-3\\xa0(Dubey et\\xa0al., 2024) tokenizer, (c & d) entropy-based patching as in this work\\xa0(¬ß2.3), (e) patching on space-bytes\\xa0(Slagle, 2024), (f) and patching on entropy using a small CNN byte-level model with 2-byte context.\\nSegmenting bytes into patches allows BLT to dynamically allocate compute based on context.\\nFigure\\xa03 shows several different methods for segmenting bytes into patches.\\nFormally, a patching function fpf_{p}italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT segments a sequence of bytes ùíô={xi,|i=1,‚Ä¶n}\\\\boldsymbol{x}=\\\\{x_{i},|i=1,\\\\ldots n\\\\}bold_italic_x = { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, | italic_i = 1, ‚Ä¶ italic_n } of length nnitalic_n into a sequence of m<nm<nitalic_m < italic_n patches ùíë={pj|j=1,‚Ä¶,m}\\\\boldsymbol{p}=\\\\{p_{j}|j=1,\\\\ldots,m\\\\}bold_italic_p = { italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_j = 1, ‚Ä¶, italic_m } by mapping each xix_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to the set {0,1} where 1 indicates the start of a new patch.\\nFor both token-based and patch-based models, the computational cost of processing data is primarily determined by the number of steps executed by the main Transformer. In BLT, this is the number of patches needed to encode the data with a given patching function.\\nConsequently, the average size of a patch, or simply patch size, is the main factor for determining the cost of processing data during both training and inference with a given patching function\\xa0(¬ß4.5).\\nNext, we introduce three patching functions: patching with a fixed number of bytes per patch\\xa0(¬ß2<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-25 08:27:30 [engine.py:317] Added request chatcmpl-82bbed6f81684165ab60079c47ce99b6.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Generating qa content from data/output/arxiv_org_2.txt...vLLM STDOUT: INFO:     127.0.0.1:37820 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 9 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_2_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_2_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†ß\u001b[0m Generating qa content from data/output/arxiv_org_2.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_2_qa_pairs.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Process 3 chunks for now -> can increase but slower!\n",
        "for filename in filenames[:3]:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create {filename} \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "    time.sleep(2) # Sleep some time to leave some room for processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNkxxvBx7Csp"
      },
      "source": [
        "Optionally, you can clean up the data via pruning \"bad\" or low quality examples and convert the rest to JSON format for finetuning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HMD-izj5OiAK"
      },
      "outputs": [],
      "source": [
        "# !synthetic-data-kit \\\n",
        "#     -c synthetic_data_kit_config.yaml \\\n",
        "#     curate --threshold 0.0 \\\n",
        "#     \"data/generated/arxiv_org_0_qa_pairs.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AScJ5-vAOjYj"
      },
      "source": [
        "We now convert the generated datasets into QA formats so we can load it for finetuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9Um4Z8SqUTB",
        "outputId": "86ce09e1-d0c4-4f51-bad0-3ac4cb80128c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/arxiv_org_0_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_0_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/arxiv_org_1_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_1_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/arxiv_org_2_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_2_qa_pairs_ft.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "qa_pairs_filenames = [\n",
        "    f\"data/generated/arxiv_org_{i}_qa_pairs.json\"\n",
        "    for i in range(len(filenames[:3]))\n",
        "]\n",
        "for filename in qa_pairs_filenames:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        save-as {filename} -f ft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dVK-qza7rPB"
      },
      "source": [
        "Let's load up the data and see what the synthetic data looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VrBwG2KT7dam"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your cleaned dataset\n",
        "DATASET_PATH = \"dataset_clean.jsonl\"\n",
        "\n",
        "# Load JSONL file into a DataFrame\n",
        "conversations = pd.read_json(DATASET_PATH, lines=True)\n",
        "\n",
        "# Optional: rename columns to standard names used in most fine-tuning notebooks\n",
        "# (The notebook usually expects `input` and `output` or `instruction` and `response`)\n",
        "conversations = conversations.rename(columns={\n",
        "    \"asm\": \"input\",\n",
        "    \"cpp\": \"output\"\n",
        "})\n",
        "\n",
        "# Remove entries missing input or output\n",
        "conversations = conversations.dropna(subset=[\"input\", \"output\"])\n",
        "\n",
        "# Create a Hugging Face Dataset object\n",
        "dataset = Dataset.from_pandas(conversations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaZ3tRP8frSn",
        "outputId": "90d7cd90-e09f-42aa-8885-3c125300a07e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unit': 'main/nw4r/ut/ut_LinkList',\n",
              " 'mangled': '__dt__Q44nw4r2ut6detail12LinkListImplFv',\n",
              " 'demangled': 'nw4r::ut::detail::LinkListImpl::~LinkListImpl()',\n",
              " 'percent': 100,\n",
              " 'src_path': 'src\\\\nw4r\\\\ut\\\\ut_LinkList.cpp',\n",
              " 'start_line': 48,\n",
              " 'end_line': 50,\n",
              " 'output': '            LinkListImpl::~LinkListImpl() {\\n                Clear();\\n            }',\n",
              " 'input': '.fn __dt__Q44nw4r2ut6detail12LinkListImplFv, global\\n\\tstwu r1, -0x10(r1)\\n\\tmflr r0\\n\\tcmpwi r3, 0x0\\n\\tstw r0, 0x14(r1)\\n\\tstw r31, 0xc(r1)\\n\\tmr r31, r3\\n\\tbeq .L_8000710C\\n\\tlwz r7, 0x4(r3)\\n\\taddi r6, r3, 0x4\\n\\tli r0, 0x0\\n\\tb .L_800070F4\\n.L_800070CC:\\n\\tlwz r8, 0x0(r7)\\n\\tlwz r5, 0x4(r7)\\n\\tstw r5, 0x4(r8)\\n\\tstw r8, 0x0(r5)\\n\\tlwz r5, 0x0(r3)\\n\\tsubi r5, r5, 0x1\\n\\tstw r5, 0x0(r3)\\n\\tstw r0, 0x0(r7)\\n\\tstw r0, 0x4(r7)\\n\\tmr r7, r8\\n.L_800070F4:\\n\\tcmplw r7, r6\\n\\tbne .L_800070CC\\n\\tcmpwi r4, 0x0\\n\\tble .L_8000710C\\n\\tmr r3, r31\\n\\tbl __dl__FPv\\n.L_8000710C:\\n\\tmr r3, r31\\n\\tlwz r31, 0xc(r1)\\n\\tlwz r0, 0x14(r1)\\n\\tmtlr r0\\n\\taddi r1, r1, 0x10\\n\\tblr\\n.endfn'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "504n46Sxfruu",
        "outputId": "288991ce-f93c-45a1-f361-eb07ca0259d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unit': 'main/nw4r/ut/ut_LinkList',\n",
              " 'mangled': 'Erase__Q44nw4r2ut6detail12LinkListImplFQ54nw4r2ut6detail12LinkListImpl8Iterator',\n",
              " 'demangled': 'nw4r::ut::detail::LinkListImpl::Erase(nw4r::ut::detail::LinkListImpl::Iterator)',\n",
              " 'percent': 100,\n",
              " 'src_path': 'src\\\\nw4r\\\\ut\\\\ut_LinkList.cpp',\n",
              " 'start_line': 25,\n",
              " 'end_line': 36,\n",
              " 'output': '            LinkListImpl::Iterator LinkListImpl::Erase(pointer p)\\n            {\\n                Node *const pNext=p->mNext,*const pPrev=p->mPrev;\\n                pNext->mPrev=pPrev;\\n                pPrev->mNext=pNext;\\n                --mSize;\\n\\n                p->mNext = nullptr;\\n                p->mPrev = nullptr;\\n                \\n                return Iterator(pNext);\\n            }',\n",
              " 'input': '.fn Erase__Q44nw4r2ut6detail12LinkListImplFQ54nw4r2ut6detail12LinkListImpl8Iterator, global\\n\\tlwz r6, 0x0(r4)\\n\\tli r0, 0x0\\n\\tlwz r5, 0x0(r6)\\n\\tb .L_8000715C\\n.L_80007134:\\n\\tlwz r7, 0x0(r6)\\n\\tlwz r4, 0x4(r6)\\n\\tstw r4, 0x4(r7)\\n\\tstw r7, 0x0(r4)\\n\\tlwz r4, 0x0(r3)\\n\\tsubi r4, r4, 0x1\\n\\tstw r4, 0x0(r3)\\n\\tstw r0, 0x0(r6)\\n\\tstw r0, 0x4(r6)\\n\\tmr r6, r7\\n.L_8000715C:\\n\\tcmplw r6, r5\\n\\tbne .L_80007134\\n\\tmr r3, r5\\n\\tblr\\n.endfn'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BVBp9YXRw_o",
        "outputId": "a95455c9-5ec0-4c22-8566-76d15dc5267a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unit': 'main/Runtime/Gecko_ExceptionPPC',\n",
              " 'mangled': '__unregister_fragment',\n",
              " 'demangled': None,\n",
              " 'percent': 100,\n",
              " 'src_path': 'src\\\\Runtime\\\\Gecko_ExceptionPPC.cpp',\n",
              " 'start_line': 28,\n",
              " 'end_line': 37,\n",
              " 'output': 'void __unregister_fragment(int id) {\\n    ProcessInfo* info;\\n\\n    if (id >= 0 && id < 1) {\\n        info = &fragmentinfo[id];\\n        info->exception_info = 0;\\n        info->TOC = 0;\\n        info->active = 0;\\n    }\\n}',\n",
              " 'input': '.fn __unregister_fragment, global\\n\\tcmpwi r3, 0x0\\n\\tbltlr\\n\\tcmpwi r3, 0x1\\n\\tbgelr\\n\\tmulli r4, r3, 0xc\\n\\tlis r3, fragmentinfo@ha\\n\\tli r0, 0x0\\n\\taddi r3, r3, fragmentinfo@l\\n\\tstwux r0, r3, r4\\n\\tstw r0, 0x4(r3)\\n\\tstw r0, 0x8(r3)\\n\\tblr\\n.endfn'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "dataset[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO9qePmP7yaY"
      },
      "source": [
        "Finally free vLLM process to save memory and to allow for finetuning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8qgTjywzgl6",
        "outputId": "70c58284-1208-4b14-c961-58b5f7a56b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to terminate the VLLM server gracefully...\n",
            "Server terminated gracefully.\n"
          ]
        }
      ],
      "source": [
        "generator.cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQo2PR7oqDQE"
      },
      "source": [
        "### Fine-tuning Synthetic Dataset with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "96416ad5-fcd1-4858-f3c6-23c769b76b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.9.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Qwen3 new models\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    # Other very popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Llama-3.2` format for conversation style finetunes. The chat template renders conversations like below: (Cutting Knowledge Date is by default there!)\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Cutting Knowledge Date: December 2023\n",
        "Today Date: 01 May 2025\n",
        "\n",
        "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "What is 1+1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "2<|eot_id|>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "579a27ac1bab4c50aa5cb200d9e2c26f",
            "8dc09ee7f1bf49eda3eab6fcbc3e7d69",
            "aaa49d93ff2442269e5860ab666e880d",
            "f4d0fa46f2a9480fafd8a48749c92b6c",
            "468e645d788146e6b9e28809ca955fec",
            "c7725636ae6b412fbaa2b9b5e68b977e",
            "e57abf0e83f24da2ac59055703af917b",
            "f7ddcd38483745e89dda6ab8036a858d",
            "a9a8ad4a74c6466bbace35c503c36c87",
            "b7a9303626294b4f86901eae9c61eba8",
            "a9b32058994a4b76bee8e6336dd9761e",
            "ff75003a5a3d4e07a8d344a1721ce39d",
            "a6c9823090b54b199ffae80a5d909738",
            "b69a126c3b1445b2b5ff605847b6551f",
            "a27d3e8aae844d4b91265a16206e50f6",
            "fa8dc8aeb14f41e3b68b5ec083d7e9bc",
            "f6b0e1431b1e4187ae91eb9cd068106a",
            "d13c1ecf17674f959eb81d105386d05c",
            "584d9081ff874a088b4f217e033e3f10",
            "3182f56a9f044a7193853d04c94e7723",
            "cbebb92aab564166932ce5ba071452d0",
            "24a398d1cb9949ae83f8eef35979fa07"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "6d09ce78-3ae1-425e-a9f4-7b3f574f94da"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9651 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "579a27ac1bab4c50aa5cb200d9e2c26f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9651 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff75003a5a3d4e07a8d344a1721ce39d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def add_messages(example):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": example[\"input\"]},\n",
        "            {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(add_messages)\n",
        "\n",
        "# ‚úÖ Now your dataset has a \"messages\" column, so the notebook code works:\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
        "        for convo in convos\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKA0VEF4CfCB"
      },
      "source": [
        "See result of the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0usAI0M40hpT",
        "outputId": "c44382c2-876e-4789-85ec-b779bf646cc5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unit': 'main/nw4r/ut/ut_LinkList',\n",
              " 'mangled': '__dt__Q44nw4r2ut6detail12LinkListImplFv',\n",
              " 'demangled': 'nw4r::ut::detail::LinkListImpl::~LinkListImpl()',\n",
              " 'percent': 100,\n",
              " 'src_path': 'src\\\\nw4r\\\\ut\\\\ut_LinkList.cpp',\n",
              " 'start_line': 48,\n",
              " 'end_line': 50,\n",
              " 'output': '            LinkListImpl::~LinkListImpl() {\\n                Clear();\\n            }',\n",
              " 'input': '.fn __dt__Q44nw4r2ut6detail12LinkListImplFv, global\\n\\tstwu r1, -0x10(r1)\\n\\tmflr r0\\n\\tcmpwi r3, 0x0\\n\\tstw r0, 0x14(r1)\\n\\tstw r31, 0xc(r1)\\n\\tmr r31, r3\\n\\tbeq .L_8000710C\\n\\tlwz r7, 0x4(r3)\\n\\taddi r6, r3, 0x4\\n\\tli r0, 0x0\\n\\tb .L_800070F4\\n.L_800070CC:\\n\\tlwz r8, 0x0(r7)\\n\\tlwz r5, 0x4(r7)\\n\\tstw r5, 0x4(r8)\\n\\tstw r8, 0x0(r5)\\n\\tlwz r5, 0x0(r3)\\n\\tsubi r5, r5, 0x1\\n\\tstw r5, 0x0(r3)\\n\\tstw r0, 0x0(r7)\\n\\tstw r0, 0x4(r7)\\n\\tmr r7, r8\\n.L_800070F4:\\n\\tcmplw r7, r6\\n\\tbne .L_800070CC\\n\\tcmpwi r4, 0x0\\n\\tble .L_8000710C\\n\\tmr r3, r31\\n\\tbl __dl__FPv\\n.L_8000710C:\\n\\tmr r3, r31\\n\\tlwz r31, 0xc(r1)\\n\\tlwz r0, 0x14(r1)\\n\\tmtlr r0\\n\\taddi r1, r1, 0x10\\n\\tblr\\n.endfn',\n",
              " 'messages': [{'content': '.fn __dt__Q44nw4r2ut6detail12LinkListImplFv, global\\n\\tstwu r1, -0x10(r1)\\n\\tmflr r0\\n\\tcmpwi r3, 0x0\\n\\tstw r0, 0x14(r1)\\n\\tstw r31, 0xc(r1)\\n\\tmr r31, r3\\n\\tbeq .L_8000710C\\n\\tlwz r7, 0x4(r3)\\n\\taddi r6, r3, 0x4\\n\\tli r0, 0x0\\n\\tb .L_800070F4\\n.L_800070CC:\\n\\tlwz r8, 0x0(r7)\\n\\tlwz r5, 0x4(r7)\\n\\tstw r5, 0x4(r8)\\n\\tstw r8, 0x0(r5)\\n\\tlwz r5, 0x0(r3)\\n\\tsubi r5, r5, 0x1\\n\\tstw r5, 0x0(r3)\\n\\tstw r0, 0x0(r7)\\n\\tstw r0, 0x4(r7)\\n\\tmr r7, r8\\n.L_800070F4:\\n\\tcmplw r7, r6\\n\\tbne .L_800070CC\\n\\tcmpwi r4, 0x0\\n\\tble .L_8000710C\\n\\tmr r3, r31\\n\\tbl __dl__FPv\\n.L_8000710C:\\n\\tmr r3, r31\\n\\tlwz r31, 0xc(r1)\\n\\tlwz r0, 0x14(r1)\\n\\tmtlr r0\\n\\taddi r1, r1, 0x10\\n\\tblr\\n.endfn',\n",
              "   'role': 'user'},\n",
              "  {'content': '            LinkListImpl::~LinkListImpl() {\\n                Clear();\\n            }',\n",
              "   'role': 'assistant'}],\n",
              " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Oct 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n.fn __dt__Q44nw4r2ut6detail12LinkListImplFv, global\\n\\tstwu r1, -0x10(r1)\\n\\tmflr r0\\n\\tcmpwi r3, 0x0\\n\\tstw r0, 0x14(r1)\\n\\tstw r31, 0xc(r1)\\n\\tmr r31, r3\\n\\tbeq .L_8000710C\\n\\tlwz r7, 0x4(r3)\\n\\taddi r6, r3, 0x4\\n\\tli r0, 0x0\\n\\tb .L_800070F4\\n.L_800070CC:\\n\\tlwz r8, 0x0(r7)\\n\\tlwz r5, 0x4(r7)\\n\\tstw r5, 0x4(r8)\\n\\tstw r8, 0x0(r5)\\n\\tlwz r5, 0x0(r3)\\n\\tsubi r5, r5, 0x1\\n\\tstw r5, 0x0(r3)\\n\\tstw r0, 0x0(r7)\\n\\tstw r0, 0x4(r7)\\n\\tmr r7, r8\\n.L_800070F4:\\n\\tcmplw r7, r6\\n\\tbne .L_800070CC\\n\\tcmpwi r4, 0x0\\n\\tble .L_8000710C\\n\\tmr r3, r31\\n\\tbl __dl__FPv\\n.L_8000710C:\\n\\tmr r3, r31\\n\\tlwz r31, 0xc(r1)\\n\\tlwz r0, 0x14(r1)\\n\\tmtlr r0\\n\\taddi r1, r1, 0x10\\n\\tblr\\n.endfn<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nLinkListImpl::~LinkListImpl() {\\n                Clear();\\n            }<|eot_id|>'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "04dbc12204fc432a8b8e36451020fb60",
            "4501640a1fd94e92affd199f49c81b74",
            "0238b85c46bd4be9ab900fb92826998e",
            "8e29cdf24d764167bc0b844ba23f5346",
            "2b2280d50578457eb462dc9b3d3c9c7b",
            "ceac01d4cb25482eb773c804e2248039",
            "c9b8359203f04893be9b0f27c12ee7e2",
            "1efd895687d746df9e4eeafdec31a443",
            "76d1a62697aa4794ac467e47c96cc492",
            "9bfd6dd999474a2fbf711d81194aca05",
            "0b67b6bf5de34ae0b5abee640c9e75ec"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "a638051e-695e-4811-b61a-34536c4cc4f9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/9651 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04dbc12204fc432a8b8e36451020fb60"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        # max_steps = None,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "        num_train_epochs= 1,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "e20559f3-e04f-46e3-de69-f20434faedb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "5.939 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "2b4ac1f3-0cb2-49ac-a427-cbdfe6e5a2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 9,651 | Num Epochs = 1 | Total steps = 1,207\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='1207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  63/1207 06:40 < 2:05:11, 0.15 it/s, Epoch 0.05/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.730300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.595100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.639800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.003100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.630300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.999400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.080100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.952000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.472000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.286800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.412100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.279200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.194600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.249200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.183800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.864000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.976200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.897400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.794700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.988000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.053700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.720300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.965400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.959900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.728300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.987100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.861100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.883500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.857600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.562900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.805400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.013900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.800900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.848100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.587400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.901600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.788700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.693700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.691300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.795500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.623000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.762200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.675100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.883900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.827100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.634900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.763200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.720500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.764300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.821700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.577600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.660600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.644700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.759700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.760400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.617900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.881100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.658700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.715000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-773422404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Return inference mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             )\n\u001b[0;32m--> 648\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    825\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "ea1594bc-57fd-4f75-d0ae-d364a935b48c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128.8037 seconds used for training.\n",
            "2.15 minutes used for training.\n",
            "Peak reserved memory = 6.795 GB.\n",
            "Peak reserved memory for training = 0.856 GB.\n",
            "Peak reserved memory % of max memory = 46.096 %.\n",
            "Peak reserved memory for training % of max memory = 5.807 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! Use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "9cfab4fd-2fb3-4051-d890-97daa1419fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bool SwitchArea::isUpdate() const {\n",
            "    if (mAreaObj->isOnGroundPlayer()) {\n",
            "        return false;\n",
            "    }\n",
            "\n",
            "    if (mAreaObj->isOnSwitchA()) {\n",
            "        return true;\n",
            "    }\n",
            "\n",
            "    return false;\n",
            "}<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \".fn isUpdate__10SwitchAreaCFv, global\\n\\tstwu r1, -0x10(r1)\\n\\tmflr r0\\n\\tstw r0, 0x14(r1)\\n\\tlwz r0, 0x20(r3)\\n\\tstw r31, 0xc(r1)\\n\\tmr r31, r3\\n\\tcmpwi r0, -0x1\\n\\tbeq .L_80025BAC\\n\\tbl isOnGroundPlayer__2MRFv\\n\\tcmpwi r3, 0x0\\n\\tbne .L_80025BAC\\n\\tli r3, 0x0\\n\\tb .L_80025BE8\\n.L_80025BAC:\\n\\tlwz r0, 0x18(r31)\\n\\tcmpwi r0, -0x1\\n\\tbeq .L_80025BC0\\n\\tli r3, 0x1\\n\\tb .L_80025BE8\\n.L_80025BC0:\\n\\tlwz r0, 0x1c(r31)\\n\\tcmpwi r0, -0x1\\n\\tbne .L_80025BE0\\n\\tmr r3, r31\\n\\tbl isOnSwitchA__7AreaObjCFv\\n\\tcntlzw r0, r3\\n\\tsrwi r3, r0, 5\\n\\tb .L_80025BE8\\n.L_80025BE0:\\n\\tmr r3, r31\\n\\tbl isOnSwitchA__7AreaObjCFv\\n.L_80025BE8:\\n\\tlwz r0, 0x14(r1)\\n\\tlwz r31, 0xc(r1)\\n\\tmtlr r0\\n\\taddi r1, r1, 0x10\\n\\tblr\\n.endfn\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
        "                   max_new_tokens = 256, temperature = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRrr--20Udm9"
      },
      "source": [
        "The model learns about the research paper!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2strt31SUc5W",
        "outputId": "4499abdc-8e33-423a-c4dc-d31739488511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BLT architecture provides improved robustness and efficiency in terms of inference and training, while maintaining state-of-the-art performance on key benchmarks.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What are some benefits of the BLT?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
        "                   max_new_tokens = 256, temperature = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "b7b69b06-5ac1-4708-9b82-6a30b3354e23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/chat_template.jinja',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "a55a8d7b-fbfe-4aa0-9a0a-05e38f8d31a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it allows for simultaneous increases in model and patch size<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is so special about BLT's tokenization?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
        "                   max_new_tokens = 256, temperature = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2aA7SECcfqJ"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf355090a6e142d38380f15b6505f2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38f2608fbba44cb3b32074f7f6ea39a1",
              "IPY_MODEL_011f35ccc7dc4b83be32265266d7c978",
              "IPY_MODEL_6886373a745c43b8a4558c6761c8e8f8"
            ],
            "layout": "IPY_MODEL_ebf8b4b97d464ee1872713fcf637fc1b"
          }
        },
        "38f2608fbba44cb3b32074f7f6ea39a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5ad5aa9f4dd4da6937b32255d5fcdde",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f418bb36d5724366a81bcd5e44a5b6aa",
            "value": "config.json:‚Äá100%"
          }
        },
        "011f35ccc7dc4b83be32265266d7c978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcecb460e3254424b2eda65376f29fbb",
            "max": 890,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45368545bf644d8e9724f95f3cad7503",
            "value": 890
          }
        },
        "6886373a745c43b8a4558c6761c8e8f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb7f86d0ff1d4b76863d1e3c3b408263",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5cc9a1e6523a4b1ebf4ec7ca0cb0d8c7",
            "value": "‚Äá890/890‚Äá[00:00&lt;00:00,‚Äá118kB/s]"
          }
        },
        "ebf8b4b97d464ee1872713fcf637fc1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5ad5aa9f4dd4da6937b32255d5fcdde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f418bb36d5724366a81bcd5e44a5b6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcecb460e3254424b2eda65376f29fbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45368545bf644d8e9724f95f3cad7503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb7f86d0ff1d4b76863d1e3c3b408263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cc9a1e6523a4b1ebf4ec7ca0cb0d8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ff4c1d1ed0c4317bb147a9af0852dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_625a537669454f3cbc4fa93a0ca0a196",
              "IPY_MODEL_2af14837a0a04742bc8e7b52a1624e8f",
              "IPY_MODEL_2d99982e163c495699f42edabf5f3668"
            ],
            "layout": "IPY_MODEL_65015e89e1d34daaa738a5d4b38ea879"
          }
        },
        "625a537669454f3cbc4fa93a0ca0a196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f64b0404e544959a6501b7c6a91d2fa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1447b6a2d32a450c8f8d24fe748b3c5b",
            "value": "tokenizer_config.json:‚Äá"
          }
        },
        "2af14837a0a04742bc8e7b52a1624e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eda89fcaeae4ecc9c347be56be61308",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_134ee758528e461f99cb2e9f4c9a5709",
            "value": 1
          }
        },
        "2d99982e163c495699f42edabf5f3668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_656c2c3e7ec8495e88c2f63cfcc1af30",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_27549249c0d74aac928aaa21b686ea14",
            "value": "‚Äá54.7k/?‚Äá[00:00&lt;00:00,‚Äá5.32MB/s]"
          }
        },
        "65015e89e1d34daaa738a5d4b38ea879": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f64b0404e544959a6501b7c6a91d2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1447b6a2d32a450c8f8d24fe748b3c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1eda89fcaeae4ecc9c347be56be61308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "134ee758528e461f99cb2e9f4c9a5709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "656c2c3e7ec8495e88c2f63cfcc1af30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27549249c0d74aac928aaa21b686ea14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ac948987aef4f00b880bff527cf766b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_abbb236ea25d47cfb4ebb5a7b1b46597",
              "IPY_MODEL_c1a7d5d7bcb944fdad8a06c148cfeb7f",
              "IPY_MODEL_441f891f7f384313b423a2a844bb6b3d"
            ],
            "layout": "IPY_MODEL_1718df89e2b0473fb1377dca84bb4b19"
          }
        },
        "abbb236ea25d47cfb4ebb5a7b1b46597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81eb748438dd4ebab05f1931e6f8afe7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3d6dc8c5e6be40e0a6316bf77145338a",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "c1a7d5d7bcb944fdad8a06c148cfeb7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6a40990f1614123a133c39ff82773ed",
            "max": 17209920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_051dc27e78f2472b94f4bde15b97fd2f",
            "value": 17209920
          }
        },
        "441f891f7f384313b423a2a844bb6b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32b5068d99004963b0ff57aad5b31404",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_31c27cc627af4493ac0924ae2a53f361",
            "value": "‚Äá17.2M/17.2M‚Äá[00:00&lt;00:00,‚Äá27.0MB/s]"
          }
        },
        "1718df89e2b0473fb1377dca84bb4b19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81eb748438dd4ebab05f1931e6f8afe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d6dc8c5e6be40e0a6316bf77145338a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6a40990f1614123a133c39ff82773ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "051dc27e78f2472b94f4bde15b97fd2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32b5068d99004963b0ff57aad5b31404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c27cc627af4493ac0924ae2a53f361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60383438655a4535b5374fbe14b984da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_347f444baaec4a54adbd53f1cff082f6",
              "IPY_MODEL_b184d42ac2c84e87b7a8e9632fcbb9ed",
              "IPY_MODEL_4c70b289d21245a9a892dbb2a298d9ff"
            ],
            "layout": "IPY_MODEL_b698b397e90f4c6a86970cbfe4b21ce2"
          }
        },
        "347f444baaec4a54adbd53f1cff082f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73d4732077f04c9990bcb66f609397b1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_92fbcf9ebb3b4ca5ba80e8c095b3efac",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "b184d42ac2c84e87b7a8e9632fcbb9ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb66a0d400db4453a2123007c0facedb",
            "max": 454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06a9e9917f6741e381a201079c627fdd",
            "value": 454
          }
        },
        "4c70b289d21245a9a892dbb2a298d9ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b50def3a36d4cf99326f571ea40cbed",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b07abf3bab734e44a51bad313c375331",
            "value": "‚Äá454/454‚Äá[00:00&lt;00:00,‚Äá24.9kB/s]"
          }
        },
        "b698b397e90f4c6a86970cbfe4b21ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d4732077f04c9990bcb66f609397b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92fbcf9ebb3b4ca5ba80e8c095b3efac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb66a0d400db4453a2123007c0facedb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06a9e9917f6741e381a201079c627fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b50def3a36d4cf99326f571ea40cbed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b07abf3bab734e44a51bad313c375331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad069762361346beaae8c9906828cb9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e826cb8408c44f9b79158d80178222a",
              "IPY_MODEL_a0abd1d66690420f86b3bb2f118317ad",
              "IPY_MODEL_0489ba3462a648a6ad6a24561943a3de"
            ],
            "layout": "IPY_MODEL_dfd0069cde014b76a9e6f0bace77c581"
          }
        },
        "7e826cb8408c44f9b79158d80178222a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b9803bbb8164596a9c3182a23698339",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3af862958cd8493185a753ee311a6cb3",
            "value": "chat_template.jinja:‚Äá"
          }
        },
        "a0abd1d66690420f86b3bb2f118317ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_319f32e7b8de4adf94cf2d85a92c15fb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43dff7b158a04c32a32ed8e52e17f19e",
            "value": 1
          }
        },
        "0489ba3462a648a6ad6a24561943a3de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_887de74451c347a89c99b87d02bd9bc9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5ff34b38d9074e2a8f8f0fe83e14a482",
            "value": "‚Äá3.83k/?‚Äá[00:00&lt;00:00,‚Äá251kB/s]"
          }
        },
        "dfd0069cde014b76a9e6f0bace77c581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9803bbb8164596a9c3182a23698339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3af862958cd8493185a753ee311a6cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "319f32e7b8de4adf94cf2d85a92c15fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "43dff7b158a04c32a32ed8e52e17f19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "887de74451c347a89c99b87d02bd9bc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ff34b38d9074e2a8f8f0fe83e14a482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "579a27ac1bab4c50aa5cb200d9e2c26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8dc09ee7f1bf49eda3eab6fcbc3e7d69",
              "IPY_MODEL_aaa49d93ff2442269e5860ab666e880d",
              "IPY_MODEL_f4d0fa46f2a9480fafd8a48749c92b6c"
            ],
            "layout": "IPY_MODEL_468e645d788146e6b9e28809ca955fec"
          }
        },
        "8dc09ee7f1bf49eda3eab6fcbc3e7d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7725636ae6b412fbaa2b9b5e68b977e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e57abf0e83f24da2ac59055703af917b",
            "value": "Map:‚Äá100%"
          }
        },
        "aaa49d93ff2442269e5860ab666e880d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7ddcd38483745e89dda6ab8036a858d",
            "max": 9651,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9a8ad4a74c6466bbace35c503c36c87",
            "value": 9651
          }
        },
        "f4d0fa46f2a9480fafd8a48749c92b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7a9303626294b4f86901eae9c61eba8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a9b32058994a4b76bee8e6336dd9761e",
            "value": "‚Äá9651/9651‚Äá[00:02&lt;00:00,‚Äá4873.98‚Äáexamples/s]"
          }
        },
        "468e645d788146e6b9e28809ca955fec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7725636ae6b412fbaa2b9b5e68b977e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e57abf0e83f24da2ac59055703af917b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7ddcd38483745e89dda6ab8036a858d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9a8ad4a74c6466bbace35c503c36c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7a9303626294b4f86901eae9c61eba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b32058994a4b76bee8e6336dd9761e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff75003a5a3d4e07a8d344a1721ce39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6c9823090b54b199ffae80a5d909738",
              "IPY_MODEL_b69a126c3b1445b2b5ff605847b6551f",
              "IPY_MODEL_a27d3e8aae844d4b91265a16206e50f6"
            ],
            "layout": "IPY_MODEL_fa8dc8aeb14f41e3b68b5ec083d7e9bc"
          }
        },
        "a6c9823090b54b199ffae80a5d909738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6b0e1431b1e4187ae91eb9cd068106a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d13c1ecf17674f959eb81d105386d05c",
            "value": "Map:‚Äá100%"
          }
        },
        "b69a126c3b1445b2b5ff605847b6551f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_584d9081ff874a088b4f217e033e3f10",
            "max": 9651,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3182f56a9f044a7193853d04c94e7723",
            "value": 9651
          }
        },
        "a27d3e8aae844d4b91265a16206e50f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbebb92aab564166932ce5ba071452d0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_24a398d1cb9949ae83f8eef35979fa07",
            "value": "‚Äá9651/9651‚Äá[00:01&lt;00:00,‚Äá4873.36‚Äáexamples/s]"
          }
        },
        "fa8dc8aeb14f41e3b68b5ec083d7e9bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6b0e1431b1e4187ae91eb9cd068106a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d13c1ecf17674f959eb81d105386d05c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "584d9081ff874a088b4f217e033e3f10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3182f56a9f044a7193853d04c94e7723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbebb92aab564166932ce5ba071452d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24a398d1cb9949ae83f8eef35979fa07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04dbc12204fc432a8b8e36451020fb60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4501640a1fd94e92affd199f49c81b74",
              "IPY_MODEL_0238b85c46bd4be9ab900fb92826998e",
              "IPY_MODEL_8e29cdf24d764167bc0b844ba23f5346"
            ],
            "layout": "IPY_MODEL_2b2280d50578457eb462dc9b3d3c9c7b"
          }
        },
        "4501640a1fd94e92affd199f49c81b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceac01d4cb25482eb773c804e2248039",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c9b8359203f04893be9b0f27c12ee7e2",
            "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=6):‚Äá100%"
          }
        },
        "0238b85c46bd4be9ab900fb92826998e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1efd895687d746df9e4eeafdec31a443",
            "max": 9651,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76d1a62697aa4794ac467e47c96cc492",
            "value": 9651
          }
        },
        "8e29cdf24d764167bc0b844ba23f5346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bfd6dd999474a2fbf711d81194aca05",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0b67b6bf5de34ae0b5abee640c9e75ec",
            "value": "‚Äá9651/9651‚Äá[00:40&lt;00:00,‚Äá283.45‚Äáexamples/s]"
          }
        },
        "2b2280d50578457eb462dc9b3d3c9c7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceac01d4cb25482eb773c804e2248039": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9b8359203f04893be9b0f27c12ee7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1efd895687d746df9e4eeafdec31a443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76d1a62697aa4794ac467e47c96cc492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bfd6dd999474a2fbf711d81194aca05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b67b6bf5de34ae0b5abee640c9e75ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}